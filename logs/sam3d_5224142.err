/dss/dsshome1/0D/ru47tac2/miniconda3/envs/sam3d/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/dss/dsshome1/0D/ru47tac2/miniconda3/envs/sam3d/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:18<09:04, 18.77s/it]  7%|▋         | 2/30 [00:19<03:45,  8.05s/it] 10%|█         | 3/30 [00:20<02:10,  4.82s/it] 13%|█▎        | 4/30 [00:20<01:21,  3.13s/it] 17%|█▋        | 5/30 [00:26<01:40,  4.02s/it] 20%|██        | 6/30 [00:26<01:07,  2.80s/it] 23%|██▎       | 7/30 [00:31<01:21,  3.53s/it] 27%|██▋       | 8/30 [00:32<00:56,  2.57s/it] 30%|███       | 9/30 [00:38<01:14,  3.54s/it] 33%|███▎      | 10/30 [00:38<00:51,  2.56s/it] 37%|███▋      | 11/30 [00:44<01:06,  3.50s/it] 40%|████      | 12/30 [00:44<00:46,  2.56s/it] 43%|████▎     | 13/30 [00:51<01:05,  3.87s/it] 47%|████▋     | 14/30 [00:51<00:44,  2.81s/it] 50%|█████     | 15/30 [00:57<00:56,  3.76s/it] 53%|█████▎    | 16/30 [00:58<00:38,  2.74s/it] 57%|█████▋    | 17/30 [01:03<00:46,  3.54s/it] 60%|██████    | 18/30 [01:03<00:30,  2.58s/it] 63%|██████▎   | 19/30 [01:09<00:37,  3.43s/it] 67%|██████▋   | 20/30 [01:09<00:25,  2.51s/it] 70%|███████   | 21/30 [01:16<00:34,  3.82s/it] 73%|███████▎  | 22/30 [01:16<00:22,  2.82s/it] 77%|███████▋  | 23/30 [01:21<00:23,  3.37s/it] 80%|████████  | 24/30 [01:21<00:14,  2.47s/it] 83%|████████▎ | 25/30 [01:28<00:18,  3.66s/it] 87%|████████▋ | 26/30 [01:28<00:10,  2.74s/it] 90%|█████████ | 27/30 [01:35<00:11,  3.76s/it] 93%|█████████▎| 28/30 [01:35<00:05,  2.75s/it] 97%|█████████▋| 29/30 [01:41<00:03,  3.58s/it]100%|██████████| 30/30 [01:41<00:00,  2.60s/it]100%|██████████| 30/30 [01:42<00:00,  3.40s/it]
Val Inference Epoch 122:   0%|          | 0/20 [00:00<?, ?it/s]Val Inference Epoch 122:   0%|          | 0/20 [00:00<?, ?it/s]
[rank0]:[W712 16:50:40.947296505 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[E712 17:00:38.546239446 ProcessGroupNCCL.cpp:632] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=8, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
[rank1]:[E712 17:00:38.546974625 ProcessGroupNCCL.cpp:2271] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 8 PG status: last enqueued work: 8, last completed work: 7
[rank1]:[E712 17:00:38.547006425 ProcessGroupNCCL.cpp:670] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E712 17:00:38.547043565 ProcessGroupNCCL.cpp:2106] [PG ID 0 PG GUID 0(default_pg) Rank 1] First PG on this rank to signal dumping.
[rank1]:[E712 17:00:39.543380545 ProcessGroupNCCL.cpp:1746] [PG ID 0 PG GUID 0(default_pg) Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 8, last completed NCCL work: 7.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E712 17:00:39.543469473 ProcessGroupNCCL.cpp:1685] [PG ID 0 PG GUID 0(default_pg) Rank 0] Observed flight recorder dump signal from another rank via TCPStore.
[rank0]:[E712 17:00:39.544724153 ProcessGroupNCCL.cpp:1746] [PG ID 0 PG GUID 0(default_pg) Rank 0] Received a dump signal due to a collective timeout from  rank 1 and we will try our best to dump the debug info. Last enqueued NCCL work: 7, last completed NCCL work: 7.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E712 17:00:39.544759279 ProcessGroupNCCL.cpp:1536] [PG ID 0 PG GUID 0(default_pg) Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[E712 17:00:39.544881671 ProcessGroupNCCL.cpp:1536] [PG ID 0 PG GUID 0(default_pg) Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank1]:[E712 17:00:39.621657231 ProcessGroupNCCL.cpp:684] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E712 17:00:39.621683671 ProcessGroupNCCL.cpp:698] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E712 17:00:39.695545238 ProcessGroupNCCL.cpp:1899] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=8, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600017 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:635 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fbfe9b785e8 in /dss/dsshome1/0D/ru47tac2/miniconda3/envs/sam3d/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x23d (0x7fbf989d3a6d in /dss/dsshome1/0D/ru47tac2/miniconda3/envs/sam3d/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0xc80 (0x7fbf989d57f0 in /dss/dsshome1/0D/ru47tac2/miniconda3/envs/sam3d/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fbf989d6efd in /dss/dsshome1/0D/ru47tac2/miniconda3/envs/sam3d/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbbf4 (0x7fc015c96bf4 in /dss/dsshome1/0D/ru47tac2/miniconda3/envs/sam3d/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7fc018ac5ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7fc018b57850 in /lib/x86_64-linux-gnu/libc.so.6)

W0712 17:00:40.254000 1831647 site-packages/torch/multiprocessing/spawn.py:169] Terminating process 1832091 via signal SIGTERM
Traceback (most recent call last):
  File "/dss/dsshome1/0D/ru47tac2/projects/segrap2025/SAM-Med3D/train.py", line 537, in <module>
    main()
  File "/dss/dsshome1/0D/ru47tac2/projects/segrap2025/SAM-Med3D/train.py", line 493, in main
    mp.spawn(main_worker, nprocs=args.world_size, args=(args, ))
  File "/dss/dsshome1/0D/ru47tac2/miniconda3/envs/sam3d/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 340, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/dss/dsshome1/0D/ru47tac2/miniconda3/envs/sam3d/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 296, in start_processes
    while not context.join():
  File "/dss/dsshome1/0D/ru47tac2/miniconda3/envs/sam3d/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 196, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with signal SIGABRT
